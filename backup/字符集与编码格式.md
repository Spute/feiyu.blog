# 前言
- 字符是人类书写系统的最小单位。包含文字、数字和符号，对人类可读。从硬件的角度理解，计算机只能识别0和1，即计算机可读的“字符”只有0和1。人类和计算机使用的字符是不一样，为了让计算机和人类能够无障碍低交流，就需要一个字符转换机制。
- 人类使用的字符数量远大于计算机使用的字符，为了做到一一对应，需要扩展计算机的字符。方法是使用多位的二进制代表更多的数字表示更多的字符。

# 字符集
计算机领域的字符集，本质是人类字符与计算机字符的映射表。
字节是计算机的最小存储单位。一个字节代表8位二进制，表示0~255数字。

为什么使用8位二进制作为基本单位？不用更多或者更少的字节？
这是历史演进的结果，早期字节代表5~7位不等，没有统一的标准。最后是权衡技术可行性、成本效益和实际需求之后的产物。


# 编码格式
是将计算机字符（数字）转换为字节序列的规则。

- 字符集和编码格式区别？
字符集定义了人类字符与计算机字符的映射。但不涉及如何表示这些字符。
编码格式则负责将字符集中的人类字符转换为具体的二进制数据，以便计算机存储和处理。


人类有不同的族群，使用不同的字符，常见的字符集：
- ASCII：一个早期的字符集，包含128个字符，包括英文字母（大小写）、数字、标点符号和控制字符。一个字节8位（表示 2^8 = 256 种不同的值），一个字节就可以表示一个ASCII字符，还多余一位。
- Unicode：一个非常大的字符集，旨在覆盖世界上所有的书写系统，为了解决国际化问题，包括各种语言的字母、符号、表情符号等。

常见的编码格式：
- ASCII 编码：使用7位二进制来编码字符集中的每个字符。例如，字母 A 编码为 0x41（十进制65）。
- UTF-8：是一种可变长度的，Unicode字符的编码格式，使用1到4个字节来表示字符。对于ASCII字符，UTF-8使用一个字节，其他字符使用多个字节。
  - UTF-8MB3：最多只支持3个字节的字符。不包含某些 emoji、罕见的汉字或其他特殊符号。如早期版的MySQL使用的就是这种编码格式。
  - UTF-8MB4 其中的 "MB4" 代表“最多使用 4 个字节”（Most Bytes 4）。它完全实现了 Unicode 字符集，包括所有需要 4 个字节来表示的字符。
  - UTF-16：也是一种 Unicode 编码格式，使用2个字节或4个字节表示字符。
  
- GBK

# 相关场景
- 代码文件开头增加# -*- coding: utf-8 -*- 的意义是？不加也可以？
  - 这行注释告诉 Python 解释器，该文件中的源代码是使用 UTF-8 编码的。
  - Python 3 默认将源代码文件视为 UTF-8 编码，因此即使不显式指定 # -*- coding: utf-8 -*-，Python 3 仍然能够正确处理文件中的非 ASCII 字符。
  - Python 2 默认将源代码文件视为 ASCII 编码

# Base64编码
- 背景是为了解决电子邮件系统的附件。起源于早期的电子邮件系统需求
由于ARPANET（互联网前身）的发展,需要解决二进制文件传输问题
早期的电子邮件和其他系统只能可靠地传输ASCII字符
一些控制字符可能会被系统误解或过滤掉
base64将二进制数据转换成可打印的ASCII字符集,确保数据不会在传输过程中被破坏

- 诞生时间早于Unicode和UTF8的产生
- 优缺点，使用编码后，数据的二进制体积增大了33%。确保数据的兼容性，但降低了性能。

- 应用场景
电子邮件附件编码(MIME)
在URL中传输二进制数据
图片等媒体文件的网络传输
XML文档中嵌入二进制数据

- 使用aes加密时，也会将加密后的内容转换成base64后，为什么常用Base64？

加密后的数据是原始字节(二进制)，这些字节可能包含任何值(0-255)在传输或存储时可能会遇到问题：
  
  - 某些系统不支持传输原始字节
  - 一些字节可能被错误解释为控制字符
  - 文本协议可能无法直接处理二进制


- url使用的编码，不是标准的base64？